# Optimizers in Deep Learning
Optimizers adjust the model's weights based on the loss function's gradients to minimize the loss

### 1. Gradient Descent
- An iterative optimization algorithm used to minimize a loss function in ML. It works by calculating the gradient (or derivative) of the function at the current point and moving in the opposite direction of the gradient to find the local minimum.
- The size of the step taken in this direction is determined by the learning rate, a hyperparameter that controls how quickly or slowly the algorithm updates the model's parameters.

**Weight Update Rule:**
$w = w - \eta \nabla L(w)$

Where:
- $w$ is the weight parameter.
- $\eta$ is the learning rate.
- $\nabla L(w)$ is the gradient of the loss function with respect to $w$.

## Steps in Gradient Descent
1. **Initialize Parameters**: Set initial values for the weights \( w \) (often random).
2. **Compute Gradient**: Calculate the gradient \( \nabla L(w) \) using the current weights.
3. **Update Weights**: Adjust the weights using the update rule \( w = w - \eta \nabla L(w) \).
4. **Iterate**: Repeat steps 2-3 until convergence (when changes in the loss function are minimal or a maximum number of iterations is reached).

## Variants of Gradient Descent

- **Batch Gradient Descent**:
  Uses the entire dataset to compute the gradient.
- **Stochastic Gradient Descent (SGD)**:
  Uses one data point at a time to update the weights, which can lead to faster convergence but more fluctuations.
- **Mini-Batch Gradient Descent**:
  Uses a small batch of data points, combining advantages of both batch and stochastic approaches.


---

### 2. Momentum

Momentum helps accelerate gradients in the right direction by adding a fraction of the previous update to the current update.

**Velocity Update Rule:**
$v = \gamma v + \eta \nabla L(w)$

**Weight Update:**
$w = w - v$

Where:
- $v$ is the velocity.
- $\gamma$ is the momentum factor, typically between 0.5 and 0.9.

---

### 3. Nesterov Accelerated Gradient (NAG)

NAG improves upon momentum by looking ahead at the future position of the parameters.

**Velocity Update:**
$v = \gamma v + \eta \nabla L(w - \gamma v)$

**Weight Update:**
$w = w - v$

---

### 4. Adaptive Gradient Algorithm (AdaGrad)

AdaGrad adapts the learning rate for each parameter based on the historical gradient values.

**Accumulated Gradient:**
$G_t = G_{t-1} + \nabla L(w_t)^2$

**Weight Update:**
$w = w - \left(\frac{\eta}{\sqrt{G_t + \epsilon}}\right) \nabla L(w)$

Where:
- $G_t$ is the sum of squares of the past gradients.
- $\epsilon$ is a small constant for numerical stability.

---

### 5. Root Mean Square Propagation (RMSprop)

RMSprop addresses AdaGrad's issue of vanishing learning rates by using an exponentially decaying average.

**Exponential Decaying Average of Squared Gradients:**
$E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma) g_t^2$

**Weight Update:**
$w = w - \left(\frac{\eta}{\sqrt{E[g^2]_t + \epsilon}}\right) \nabla L(w)$

---

### 6. Adaptive Moment Estimation (Adam)

Adam combines the advantages of ***AdaGrad*** and ***RMSprop***.

**Moment Estimates:**
$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$

$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$

**Bias-corrected Estimates:**
$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$

$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$

**Weight Update:**
$w = w - \left(\frac{\eta}{\sqrt{\hat{v}_t} + \epsilon}\right) \hat{m}_t$

Where:
- $m_t$ and $v_t$ are the first and second moment estimates.
- $\beta_1$ and $\beta_2$ are hyperparameters for moment decay rates.

---

### 7. AdamW (Adam with Weight Decay)

AdamW improves Adam by decoupling the weight decay term from the gradient update.

**Weight Update with Weight Decay:**
$w = w - \eta \left(\frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda w\right)$

Where $\lambda$ is the weight decay coefficient.

---

### 8. AdaDelta

AdaDelta uses a moving window of gradients instead of accumulating all past gradients.

**Accumulated Gradients:**
$E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma) g_t^2$

**Weight Update:**
$\Delta w_t = - \left(\frac{\sqrt{E[\Delta w^2]_{t-1} + \epsilon}}{\sqrt{E[g^2]_t + \epsilon}}\right) g_t$

$w = w + \Delta w_t$

---

### 9. AMSGrad

AMSGrad ensures the learning rate doesn't increase by maintaining a maximum of past squared gradients.

**Maximum Squared Gradient:**
$\hat{v}_t = \max(\hat{v}_{t-1}, v_t)$

**Weight Update:**
$w = w - \left(\frac{\eta}{\sqrt{\hat{v}_t} + \epsilon}\right) \hat{m}_t$

---

### 10. Nadam

Nadam combines Adam and Nesterov momentum.

**Weight Update Rule:**
$w = w - \left(\frac{\eta}{\sqrt{\hat{v}_t} + \epsilon}\right) \left(\beta_1 \hat{m}_t + \frac{(1 - \beta_1) g_t}{1 - \beta_1^t}\right)$

---

### 11. Lookahead Optimizer

Lookahead maintains a "fast" and "slow" set of weights, updating the "slow" weights based on the average of the "fast" weights.

There are no specific equations, as Lookahead involves an inner and outer loop mechanism.

---

### 12. LAMB (Layer-wise Adaptive Moments for Batch training)

LAMB scales layer-wise learning rates based on the norm of the gradients and weights.

**Layer-wise Scaling:**
$r_t = \text{clip}\left(\frac{||w||}{||g_t||}, \text{min}, \text{max}\right)$

**Weight Update:**
$w = w - \eta \left(\frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}\right) r_t$

---

# Summary
For most deep learning tasks, **Adam** and **AdamW** are popular choices due to their adaptive learning rates. **SGD with Momentum** works well for simpler tasks and traditional computer vision problems. Techniques like **NAG**, **AdaDelta**, and **Lookahead** can offer improvements in specific scenarios.
